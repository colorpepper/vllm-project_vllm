root@09afcccf4e0a:/home/git/rocm_vllm/benchmarks# python -m vllm.entrypoints.openai.api_server --model /home/huggingface.co/facebook_opt-125m/   --swap-space 16 --disable-log-requests
INFO 06-11 03:53:20 api_server.py:719] args: Namespace(host=None, port=8000, allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], served_model_name=None, chat_template=None, response_role='assistant', model='/home/huggingface.co/facebook_opt-125m/', tokenizer=None, revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, block_size=16, seed=0, swap_space=16, gpu_memory_utilization=0.9, max_num_batched_tokens=None, max_num_seqs=256, max_paddings=256, disable_log_stats=False, quantization=None, engine_use_ray=False, disable_log_requests=True, max_log_len=None)
INFO 06-11 03:53:20 llm_engine.py:73] Initializing an LLM engine with config: model='/home/huggingface.co/facebook_opt-125m/', tokenizer='/home/huggingface.co/facebook_opt-125m/', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=pt, tensor_parallel_size=1, quantization=None, seed=0)
WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:
    PyTorch 2.1.1+cu121 with CUDA 1201 (you have 2.0.1+gita61a294)
    Python  3.10.13 (you have 3.10.13)
  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)
  Memory-efficient attention, SwiGLU, sparse and more won't be available.
  Set XFORMERS_MORE_DETAILS=1 for more details
MegaBlocks not found. Please install it by `pip install megablocks`.
STK not found: please see https://github.com/stanford-futuredata/stk
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML
  warnings.warn("Can't initialize NVML")
Traceback (most recent call last):
  File "/opt/conda/envs/py_3.10/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/envs/py_3.10/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/vllm-0.2.4+rocm573-py3.10-linux-x86_64.egg/vllm/entrypoints/openai/api_server.py", line 729, in <module>
    engine = AsyncLLMEngine.from_engine_args(engine_args)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/vllm-0.2.4+rocm573-py3.10-linux-x86_64.egg/vllm/engine/async_llm_engine.py", line 495, in from_engine_args
    engine = cls(parallel_config.worker_use_ray,
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/vllm-0.2.4+rocm573-py3.10-linux-x86_64.egg/vllm/engine/async_llm_engine.py", line 269, in __init__
    self.engine = self._init_engine(*args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/vllm-0.2.4+rocm573-py3.10-linux-x86_64.egg/vllm/engine/async_llm_engine.py", line 314, in _init_engine
    return engine_class(*args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/vllm-0.2.4+rocm573-py3.10-linux-x86_64.egg/vllm/engine/llm_engine.py", line 112, in __init__
    self._init_cache()
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/vllm-0.2.4+rocm573-py3.10-linux-x86_64.egg/vllm/engine/llm_engine.py", line 208, in _init_cache
    num_blocks = self._run_workers(
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/vllm-0.2.4+rocm573-py3.10-linux-x86_64.egg/vllm/engine/llm_engine.py", line 750, in _run_workers
    self._run_workers_in_batch(workers, method, *args, **kwargs))
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/vllm-0.2.4+rocm573-py3.10-linux-x86_64.egg/vllm/engine/llm_engine.py", line 724, in _run_workers_in_batch
    output = executor(*args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/vllm-0.2.4+rocm573-py3.10-linux-x86_64.egg/vllm/worker/worker.py", line 86, in profile_num_available_blocks
    self.model_runner.profile_run()
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/vllm-0.2.4+rocm573-py3.10-linux-x86_64.egg/vllm/worker/model_runner.py", line 321, in profile_run
    self.execute_model(seqs, kv_caches)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/vllm-0.2.4+rocm573-py3.10-linux-x86_64.egg/vllm/worker/model_runner.py", line 279, in execute_model
    hidden_states = self.model(
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/vllm-0.2.4+rocm573-py3.10-linux-x86_64.egg/vllm/model_executor/models/opt.py", line 313, in forward
    hidden_states = self.model(input_ids, positions, kv_caches,
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/vllm-0.2.4+rocm573-py3.10-linux-x86_64.egg/vllm/model_executor/models/opt.py", line 287, in forward
    return self.decoder(input_ids, positions, kv_caches, input_metadata,
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/vllm-0.2.4+rocm573-py3.10-linux-x86_64.egg/vllm/model_executor/models/opt.py", line 259, in forward
    hidden_states = layer(hidden_states, kv_caches[i], input_metadata,
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/vllm-0.2.4+rocm573-py3.10-linux-x86_64.egg/vllm/model_executor/models/opt.py", line 164, in forward
    hidden_states = self.self_attn(hidden_states=hidden_states,
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/vllm-0.2.4+rocm573-py3.10-linux-x86_64.egg/vllm/model_executor/models/opt.py", line 106, in forward
    attn_output = self.attn(q, k, v, key_cache, value_cache,
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/vllm-0.2.4+rocm573-py3.10-linux-x86_64.egg/vllm/model_executor/layers/attention.py", line 155, in forward
    out = xops.memory_efficient_attention_forward(
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/xformers/ops/fmha/__init__.py", line 244, in memory_efficient_attention_forward
    return _memory_efficient_attention_forward(
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/xformers/ops/fmha/__init__.py", line 341, in _memory_efficient_attention_forward
    out, *_ = op.apply(inp, needs_gradient=False)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/xformers/ops/fmha/flash.py", line 458, in apply
    out, softmax_lse, rng_state = cls.OPERATOR(
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/xformers/ops/fmha/flash.py", line 130, in _flash_fwd
    ) = _C_flashattention.varlen_fwd(
RuntimeError: FlashAttention only supports AMD MI200 GPUs or newer.
root@09afcccf4e0a:/home/git/rocm_vllm/benchmarks#




----------------------------------------------------------------------------------------------

root@ubt2204s-uefi-amd-diag:/mnt/git/rocm_vllm# python -m vllm.entrypoints.openai.api_server --model facebook/opt-125m
/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
INFO 06-12 14:03:50 llm_engine.py:103] Initializing an LLM engine (v0.4.2) with config: model='facebook/opt-125m', speculative_config=None, tokenizer='facebook/opt-125m', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=facebook/opt-125m)
Traceback (most recent call last):
  File "/usr/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/usr/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/mnt/git/rocm_vllm/vllm/entrypoints/openai/api_server.py", line 186, in <module>
    engine = AsyncLLMEngine.from_engine_args(
  File "/mnt/git/rocm_vllm/vllm/engine/async_llm_engine.py", line 374, in from_engine_args
    engine = cls(
  File "/mnt/git/rocm_vllm/vllm/engine/async_llm_engine.py", line 328, in __init__
    self.engine = self._init_engine(*args, **kwargs)
  File "/mnt/git/rocm_vllm/vllm/engine/async_llm_engine.py", line 450, in _init_engine
    return engine_class(*args, **kwargs)
  File "/mnt/git/rocm_vllm/vllm/engine/llm_engine.py", line 165, in __init__
    self.model_executor = executor_class(
  File "/mnt/git/rocm_vllm/vllm/executor/executor_base.py", line 41, in __init__
    self._init_executor()
  File "/mnt/git/rocm_vllm/vllm/executor/gpu_executor.py", line 22, in _init_executor
    self.driver_worker = self._create_worker()
  File "/mnt/git/rocm_vllm/vllm/executor/gpu_executor.py", line 67, in _create_worker
    wrapper.init_worker(**self._get_worker_kwargs(local_rank, rank,
  File "/mnt/git/rocm_vllm/vllm/worker/worker_base.py", line 131, in init_worker
    self.worker = worker_class(*args, **kwargs)
  File "/mnt/git/rocm_vllm/vllm/worker/worker.py", line 74, in __init__
    self.model_runner = ModelRunnerClass(
  File "/mnt/git/rocm_vllm/vllm/worker/model_runner.py", line 114, in __init__
    self.attn_backend = get_attn_backend(
  File "/mnt/git/rocm_vllm/vllm/attention/selector.py", line 33, in get_attn_backend
    backend = _which_attn_to_use(num_heads, head_size, num_kv_heads,
  File "/mnt/git/rocm_vllm/vllm/attention/selector.py", line 95, in _which_attn_to_use
    if torch.cuda.get_device_capability()[0] < 8:
  File "/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py", line 430, in get_device_capability
    prop = get_device_properties(device)
  File "/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py", line 444, in get_device_properties
    _lazy_init()  # will define _get_device_properties
  File "/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py", line 293, in _lazy_init
    torch._C._cuda_init()
RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
root@ubt2204s-uefi-amd-diag:/mnt/git/rocm_vllm#


-------------------------------------------------------------------------------------------------

